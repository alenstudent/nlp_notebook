# 概率论基本概念

## 概率
​		概率（probability）是从随机试验中的事件到实数域的映射函数，用以表示事件发生的可能性。如果用$P(A)$作为事件A的概率, $\Omega$是试验的样本空间，则概率函数必须满足以下三条公理：

​		**公理1**（非负性）：$P(A) > 0$

​		**公理2**（规范性）：$P(\Omega) = 1$

​		**公理3**（可列可加性）：对于可列无穷多个事件$A_1, A_2,...,A_n$，如果事件两两互不相容，即对任意的$i$和$j(i\neq j)$，事件$A_i$和$A_j$不相交（$A_i \bigcap A_j = \emptyset$ ）,则有
$$
P(\bigcup_{i=0}^{\infty}A_i) = \sum_{i=0}^{\infty}P(A_i)
$$

### 最大似然估计

​		如果${s_1, s_2, ..., s_n}$是一个试验的样本空间，在相同的情况下重复试验$N$次，观察到样本$s_k(1 \leq k \leq n)$的次数为$n_N(s_k)$，那么，$s_k$在这$N$次试验中的相对频率为
$$
q_N(s_k) = \frac{n_N(s_k)}{N}
$$
由于$\sum_{k = 1}^{n}n_N(s_k) = N$， 因此$\sum_{k = 1}^{n}q_N(s_k) = 1$



### 熵

​	熵又称自信息（self-information）, 可以视为描述一个随机变量的不确定性的数量。它表示信源每发一个符号所提供的平均信息量。一个随机变量的熵越大，它的不确定性越大，那么，正确估计其值的可能性就越小。越不确定的随机变量越需要大的信息量用以确定其值。

​	如果X是一个离散型随机变量，取值空间为$R$,其概率分页为$p(x) = P(X = x), x \in R$。那么，$X$的熵为$H(X)$定义为：
$$
H(X) = -\sum_{x \in R}p(x)log_{2}p(x)
$$

### 联合熵

​	如果$X, Y$是一对离散型随机变量$X, Y$ ~ $p(x, y)$，$X, Y$的联合熵（joint entropy）$H(X, Y)$定义为
$$
H(X, Y) = -\sum_{x \in X}\sum_{y \in Y}p(x, y)logp(x,y)
$$

### 条件熵

​	给定随机变量X的情况下，随机变量Y的条件熵（conditional entropy）定义为：
$$
\begin{equation}
\begin{aligned}
H(Y|X) &= \sum_{x \in X}p(x)H(Y|X=x) \\
& = \sum_{x \in X}p(x)[-\sum_{y \in Y}p(y|x)logp(y|x)] \\
&=-\sum_{x \in X}\sum_{y \in Y}p(x,y)logp(y|x)
 \end{aligned}
\end{equation}
$$

### 联合熵的推导公式

$$
\begin{equation}
\begin{aligned}
H(X,Y) &= -\sum_{x \in X}\sum_{y \in Y}p(x, y)logp(x, y) \\
& =-\sum_{x \in X}\sum_{y \in Y}p(x, y)log[p(x)p(y|x)] \\
& = -\sum_{x \in X}\sum_{y \in Y}p(x,y)[logp(x)+logp(y|x)] \\
& = -\sum_{x \in X}\sum_{y \in Y}p(x, y)logp(x) - \sum_{x \in X}\sum_{y \in Y}p(x, y)logp(y|x) \\ 
& = -\sum_{x \in X}p(x)logp(x) - \sum_{x \ in X}\sum_{y \in Y}p(x, y)logp(y|x) \\ 
& = H(X) + H(Y|X)
\end{aligned}
\end{equation}
$$

